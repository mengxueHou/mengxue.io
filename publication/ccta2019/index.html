<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: January 9, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/www.mengxuehou.com/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/www.mengxuehou.com/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/www.mengxuehou.com/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/www.mengxuehou.com/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Mengxue Hou"><meta name=description content="We investigate interaction  between human and a miniature autonomous blimp, by letting the human  control position of the blimp through pointing motion. The blimp is controlled by a position feedback controller, with the reference position set to the position of pointer. We observe that the blimp can follow the  pointing motion, and reach certain target position. Since the human intention represented by the desired target position for the blimp is not measurable during the process, the Vector Integration to Endpoint (VITE) model is applied to model the dynamics of human pointing motion and  to identify the hidden human intention. Stability analysis shows that the closed-loop human-blimp dynamics are exponentially stable. Experimental data verifies that the VITE model is applicable to model human blimp interaction in 3D space, and the human intention can be identified from trajectories of the blimp and pointer movements."><link rel=alternate hreflang=en-us href=www.mengxuehou.com/publication/ccta2019/><link rel=canonical href=www.mengxuehou.com/publication/ccta2019/><link rel=manifest href=www.mengxuehou.com/manifest.webmanifest><link rel=icon type=image/png href=/www.mengxuehou.com/media/icon_huc2f53cd1982dbfa0553f7dd5ee6f1c26_10698_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/www.mengxuehou.com/media/icon_huc2f53cd1982dbfa0553f7dd5ee6f1c26_10698_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="/www.mengxuehou.com/publication/ccta2019/featured.png"><meta property="og:site_name" content="Academic"><meta property="og:url" content="www.mengxuehou.com/publication/ccta2019/"><meta property="og:title" content="Modeling Pointing Tasks in Human-Blimp Interactions | Academic"><meta property="og:description" content="We investigate interaction  between human and a miniature autonomous blimp, by letting the human  control position of the blimp through pointing motion. The blimp is controlled by a position feedback controller, with the reference position set to the position of pointer. We observe that the blimp can follow the  pointing motion, and reach certain target position. Since the human intention represented by the desired target position for the blimp is not measurable during the process, the Vector Integration to Endpoint (VITE) model is applied to model the dynamics of human pointing motion and  to identify the hidden human intention. Stability analysis shows that the closed-loop human-blimp dynamics are exponentially stable. Experimental data verifies that the VITE model is applicable to model human blimp interaction in 3D space, and the human intention can be identified from trajectories of the blimp and pointer movements."><meta property="og:image" content="/www.mengxuehou.com/publication/ccta2019/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2019-07-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"www.mengxuehou.com/publication/ccta2019/"},"headline":"Modeling Pointing Tasks in Human-Blimp Interactions","image":["/www.mengxuehou.com/publication/ccta2019/featured.png"],"datePublished":"2019-01-01T00:00:00Z","dateModified":"2019-07-01T00:00:00Z","author":{"@type":"Person","name":"Mengxue Hou"},"publisher":{"@type":"Organization","name":"Academic","logo":{"@type":"ImageObject","url":"/www.mengxuehou.com/media/icon_huc2f53cd1982dbfa0553f7dd5ee6f1c26_10698_192x192_fill_lanczos_center_3.png"}},"description":"We investigate interaction  between human and a miniature autonomous blimp, by letting the human  control position of the blimp through pointing motion. The blimp is controlled by a position feedback controller, with the reference position set to the position of pointer. We observe that the blimp can follow the  pointing motion, and reach certain target position. Since the human intention represented by the desired target position for the blimp is not measurable during the process, the Vector Integration to Endpoint (VITE) model is applied to model the dynamics of human pointing motion and  to identify the hidden human intention. Stability analysis shows that the closed-loop human-blimp dynamics are exponentially stable. Experimental data verifies that the VITE model is applicable to model human blimp interaction in 3D space, and the human intention can be identified from trajectories of the blimp and pointer movements."}</script><title>Modeling Pointing Tasks in Human-Blimp Interactions | Academic</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=72210adeb1424686b1bcc444bd2d48a0><script src=/www.mengxuehou.com/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Academic</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Academic</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/www.mengxuehou.com/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://www.researchgate.net/profile/Mengxue-Hou data-toggle=tooltip data-placement=bottom title=ResearchGate target=_blank rel=noopener aria-label=ResearchGate><i class="fab fa-researchgate" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Modeling Pointing Tasks in Human-Blimp Interactions</h1><div class=article-metadata><div><span class=author-highlighted>Mengxue Hou</span>, <span>Qiuyang Tao</span>, <span>Paul Varnell</span>, <span>Fumin Zhang</span></div><span class=article-date>July, 2019</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=/www.mengxuehou.com/publication/ccta2019/CCTA2019.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/www.mengxuehou.com/publication/ccta2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://www.youtube.com/4JavPaOVKio target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/mengxueHou/human-blimp-interaction/blob/b27c2e0cbcb3931a8932efeb3d13a74ddcb1806c/README.md target=_blank rel=noopener>Source Document</a>
<a class="btn btn-outline-primary btn-page-header" href=https://doi.org/10.1109/CCTA.2019.8920528 target=_blank rel=noopener>DOI</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:466px><div style=position:relative><img src=/www.mengxuehou.com/publication/ccta2019/featured_hub9ddfbb2873ab32db493254c1beb812b_616894_720x2500_fit_q75_h2_lanczos_3.webp width=720 height=466 alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>We investigate interaction between human and a miniature autonomous blimp, by letting the human control position of the blimp through pointing motion. The blimp is controlled by a position feedback controller, with the reference position set to the position of pointer. We observe that the blimp can follow the pointing motion, and reach certain target position. Since the human intention represented by the desired target position for the blimp is not measurable during the process, the Vector Integration to Endpoint (VITE) model is applied to model the dynamics of human pointing motion and to identify the hidden human intention. Stability analysis shows that the closed-loop human-blimp dynamics are exponentially stable. Experimental data verifies that the VITE model is applicable to model human blimp interaction in 3D space, and the human intention can be identified from trajectories of the blimp and pointer movements.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=www.mengxuehou.com/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>Proc. of the 3rd IEEE Conference on Control Technology and Applications</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=www.mengxuehou.com/tag/human-robot-interaction/>human robot interaction</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=www.mengxuehou.com%2Fpublication%2Fccta2019%2F&text=Modeling+Pointing+Tasks+in+Human-Blimp+Interactions" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=www.mengxuehou.com%2Fpublication%2Fccta2019%2F&t=Modeling+Pointing+Tasks+in+Human-Blimp+Interactions" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Modeling%20Pointing%20Tasks%20in%20Human-Blimp%20Interactions&body=www.mengxuehou.com%2Fpublication%2Fccta2019%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=www.mengxuehou.com%2Fpublication%2Fccta2019%2F&title=Modeling+Pointing+Tasks+in+Human-Blimp+Interactions" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Modeling+Pointing+Tasks+in+Human-Blimp+Interactions%20www.mengxuehou.com%2Fpublication%2Fccta2019%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=www.mengxuehou.com%2Fpublication%2Fccta2019%2F&title=Modeling+Pointing+Tasks+in+Human-Blimp+Interactions" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=www.mengxuehou.com/><img class="avatar mr-3 avatar-circle" src=/www.mengxuehou.com/authors/admin/avatar_hua1cc55be24b3c9217e727c8efb4058ae_2261412_270x270_fill_q75_lanczos_center.JPG alt="Mengxue Hou"></a><div class=media-body><h5 class=card-title><a href=www.mengxuehou.com/>Mengxue Hou</a></h5><h6 class=card-subtitle>Lillian Gilbreth Postdoc Fellow</h6><p class=card-text>My research interests include robotic autonomy, mobile sensor networks, and human robot interaction. I aim to devise practical, computationally-efficient, and provably-correct algorithms that prepare robotic systems to be <strong>cognizant</strong>, <strong>taskable</strong>, and <strong>adaptive</strong>, and can collaborate with human operators to co-exist in a complex, ever-changing and unknown environment.</p><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://www.researchgate.net/profile/Mengxue-Hou target=_blank rel=noopener><i class="fab fa-researchgate"></i></a></li><li><a href="https://scholar.google.com/citations?user=HpMCu8kAAAAJ&hl=en" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/mengxueHou/ target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/mengxue-hou-384240a8/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/www.mengxuehou.com/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/www.mengxuehou.com/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/www.mengxuehou.com/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/www.mengxuehou.com/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>